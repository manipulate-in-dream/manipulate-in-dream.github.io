<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Unified Visual Imagination and Control via Hierarchical World Model.">
  <meta name="keywords" content="MinD, World Model, Video Generation, Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MinD: Unified Visual Imagination and Control via Hierarchical World Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MinD: Unified Visual Imagination and Control via Hierarchical World Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/litwellchi">Xiaowei Chi</a><sup>1,2</sup><sup><sup>*</sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://kuangzhige.github.io">Kuangzhi Ge</a><sup>3</sup><sup><sup>*</sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://liujiaming1996.github.io/">Jiaming Liu</a><sup>3</sup><sup><sup>&#8224;</sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://rainbow979.github.io/">Siyuan Zhou</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Wrcer2IAAAAJ&hl=zh-CN">Peidong Jia</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Zichen He<sup>3</sup>,
            </span>
            <br>
            <span class="author-block">
              Yuzhen Liu<sup>1</sup>
            </span>
            <span class="author-block">
              Tingguang Li<sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://facultyprofiles.hkust.edu.hk/profiles.php?profile=sirui-han-siruihan">Sirui Han</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a><sup>3</sup><sup><sup>&#9993;</sup></sup>
            </span>
            <span class="author-block">
              <a href="https://facultyprofiles.hkust.edu.hk/profiles.php?profile=yike-guo-yikeguo">Yike Guo</a><sup>2</sup><sup><sup>&#9993;</sup></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tencent RoboticsX,</span>
            <span class="author-block"><sup>2</sup>Hong Kong University of Science and Technology</span>
            <span class="author-block"><sup>3</sup>Peking University</span>
            <span class="author-block"><sup>*</sup>Equal contribution</span>
            <span class="author-block"><sup><sup>&#8224;</sup></sup>Project Lead</span>
            <span class="author-block"><sup><sup>&#9993;</sup></sup>Corresponding Author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.jpg" alt="Teaser Image" width="100%" />
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">MinD</span> is a hierarchical world model that unifies visual imagination and control
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<style>
  section {
    margin-bottom: -50px; /* Adjust this value to reduce vertical spacing */
  }
</style>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video generation models (VGMs) offer a promising pathway for unified world modeling in robotics by integrating simulation, prediction, and manipulation. However, their practical application remains limited due to (1) slowgeneration speed, which limits real-time interaction, and (2) poor consistency between imagined videos and executable actions.
          </p>
          <p>
            To address these challenges, we propose <strong>Manipulate in Dream (MinD)</strong>, a hierarchical diffusion-based world model framework that employs a dual-system design for vision-language manipulation. MinD executes VGM at low frequencies to extract video prediction features, while leveraging a high-frequency diffusion policy for real-time interaction. This architecture enables low-latency, closed-loop control in manipulation with coherent visual guidance.
          </p>
          <p>
            To better coordinate the two systems, we introduce a video-action diffusion matching module (DiffMatcher), with a novel co-training strategy that uses separate schedulers for each diffusion model. Specifically, we introduce a diffusion-forcing mechanism to DiffMatcher that aligns their intermediate representations during training, helping the fast action model better understand video-based predictions. Beyond manipulation, MinD also functions as a world simulator, reliably predicting task success or failure in latent space before execution. Trustworthy analysis further shows that VGMs can preemptively evaluate task feasibility and mitigate risks. Extensive experiments across multiple benchmarks demonstrate that MinD achieves state-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of unified world modeling in robotics.
            </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
      <div class="container is-max-desktop">
        <h2 class="title is-3">The MinD Model</h2>
        <div class="publication-image">
          <img src="./static/images/method_main.jpg" alt="Overview Image" style="width:100%;"/>
        </div>
      </div>
      <div class="content has-text-justified">
        <p>MinD is a general-purpose multimodal world model for robotic manipulation that integrates visual imagination and action planning. Its core consists of a hierarchical diffusion-based framework with three components:
        </p>
        <ol>
          <li><b>LoDiff-Visual (Slow System):</b> Generates a sequence of future visual observations at a low temporal frequency
            using a latent diffusion model, focusing on long-horizon imagination.</li>
          <li><b>HiDiff-Policy (Fast System):</b> Predicts high-frequency action sequences from the generated video rollout
            using a high-frequency diffusion transformer, ensuring real-time responsiveness.</li>
          <li><b>Video-Action DiffMatcher:</b> A temporal alignment module that bridges the asynchronous generation by
            converting latent video tensors into temporally-aware visual tokens, which then condition the HiDiff-Policy.</li>
        </ol>
        <p>During inference, LoDiff-Visual forward-simulates noisy visual latents, which DiffMatcher transforms into aligned
          features to condition HiDiff-Policy for action generation. The system is trained with a dual-scheduler co-training
          strategy, optimizing a total objective that includes a video loss, an action loss, and a regularization loss for
          DiffMatcher (to enforce consistency between noisy and clean visual features), ensuring robust performance across
          asynchronous temporal scales and imperfect inputs.
        </p>
        </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="column is-full-width">
      <h3 class="title is-3" style="margin-top: -30pt;">Experimental Results</h3>

      <h3 class="title is-4">Evaluation on RL-Bench</h3>
      <div class="content has-text-justified has-text-centered">
        <p>
          We first evaluate MinD in the <a href="https://github.com/stepjam/RLBench" target="_blank">RL-Bench</a> evaluation environment. This simulation platform is a comprehensive robot learning benchmark and environment with 7 tasks designed to advance research in vision-guided robot manipulation with a single-arm Franka Panda robot and a front-view camera. We compare MinD with existing VLA models, including <a href="https://arxiv.org/abs/2411.19650" target="_blank">CogACT</a>, <a href="https://arxiv.org/abs/2406.04339" target="_blank">RoboMamba</a>, <a href="https://arxiv.org/html/2404.12377v1" target="_blank">RoboDreamer</a>, and <a href="https://arxiv.org/abs/2406.09246" target="_blank">OpenVLA</a>. For the model using Mamba or LLM as the backbone, we colored it with a green background. We use a yellow background for the VLA models with a video generation backbone, and a red background for our method.
        </p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
          <tr>
            <td width="12%"></td>
            <td width="76%">
              <img src="static/images/exp_rlbench.png" />
              <p class="has-text-centered" style="margin-top: -0.5em;">
                Evaluation and comparison on RL-Bench tasks. All models are finetuned on the collected 1000 trajectories <span style="font-size: 80%;">(including 100 trajectories for each 7 tasks and 300 more randomly sampled tasks).</span>
              </p>
            </td>
            <td width="12%"></td>
          </tr>
        </table>
        <p>
            The results show that MinD outperforms all the existing VLA models, especially in tasks requiring complex temporal reasoning, such as "<b>Sweep to Dustpan</b>" (<span style="font-weight:bold;">96%</span>) and "<b>Close Laptop Lid</b>" (<span style="font-weight:bold;">68%</span>), highlighting the strong capability of video generation models as the foundational backbone for comprehensive visual-language manipulations.
        </p>
      </div>

      <h3 class="title is-4">Real-world Evaluation with Franka Research 3 Robot</h3>
      <div class="content has-text-justified has-text-centered">
        <p>
            We evaluate MinD with a <a href="https://franka.de/franka-research-3" target="_blank">Franka Research 3 Robot</a> to perform 4 real-world tasks: 1) pick and place, 2) unplug the charger, 3)
            pour water, 4) wipe the whiteboard. We collected a dataset with 100 human demonstration trajectories via teleoperation using a SpaceMouse. As shown in the table below, our model achieves competitive performance across all tasks, with notable strengths in tasks requiring precise manipulation, such as pick and place(60%) and wiping the whiteboard (65%).
        </p>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
          <tr>
            <td width="9%"></td>
            <td width="82%">
              <img src="./static/images/exp_franka.png" />
              <p class="has-text-centered" style="margin-top: -0.5em;"> Real-world evaluation with the Franka robot across four tasks, each with 20 trials of random configurations.</p>
            </td>
            <td width="9%"></td>
          </tr>
        </table>
      </div>

      <h3 class="title is-3" style="margin-top: 30pt;">Ablation Study</h3>
      <h4 class="title is-5">Action Model Scaling</h4>
      <p>
        We evaluate various action model architectures on Google Robot (GR) and WidowX Robot (WR) in SIMPLER. The
        architectures
        examined include MLPs with depths of 3 and 7 layers, respectively, as well as a
        series of DiT of varying sizes. The results show that both MLP and DiT structures show improved success rates
        with increased model
        size, and DiT significantly outperforms MLP. Notably, DiT-Large achieves the highest average success
        rate of 64.8%. The average success rate of transformers is approximately linearly related to the logarithm
        of the model size, indicating a favorable scaling behavior of the action module with diffusion
        transformers.
      </p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
        <tr>
          <td width="25%"></td>
          <td width="45%">
            <img src="static/images/ablation-model_size.png" />
          </td>
          <td width="25%"></td>
        </tr>
      </table>
      <h4 class="title is-5">Adaptive Action Ensemble</h4>
      <p>
        We evaluate the proposed Adaptive Action Ensemble approach against the two ensemble strategies introduced in
        <a href="https://arxiv.org/abs/2304.13705" target="_blank">ACT</a> – Action Chunking and Temporal Ensemble. The
        results show that
        our proposed Adaptive Ensemble outperforms others, and we attribute this to the efficacy of our adaptive
        similarity
        weighting between the current and historical predictions.
      </p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
        <tr>
          <td width="25%"></td>
          <td width="42%">
            <img src="static/images/ablation-action.png" />
          </td>
          <td width="25%"></td>
        </tr>
      </table>
      <br />


    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
