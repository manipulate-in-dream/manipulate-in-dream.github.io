<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Unified Visual Imagination and Control via Hierarchical World Model.">
  <meta name="keywords" content="MinD, World Model, Video Generation, Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MinD: Unified Visual Imagination and Control via Hierarchical World Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MinD: Unified Visual Imagination and Control via Hierarchical World Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/litwellchi">Xiaowei Chi</a><sup>1,2</sup><sup><sup>*</sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://kuangzhige.github.io">Kuangzhi Ge</a><sup>3</sup><sup><sup>*</sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://liujiaming1996.github.io/">Jiaming Liu</a><sup>3</sup><sup><sup>&#8224;</sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://rainbow979.github.io/">Siyuan Zhou</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Wrcer2IAAAAJ&hl=zh-CN">Peidong Jia</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Zichen He<sup>3</sup>,
            </span>
            <br>
            <span class="author-block">
              Yuzhen Liu<sup>1</sup>
            </span>
            <span class="author-block">
              Tingguang Li<sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://facultyprofiles.hkust.edu.hk/profiles.php?profile=sirui-han-siruihan">Sirui Han</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a><sup>3</sup><sup><sup>&#9993;</sup></sup>
            </span>
            <span class="author-block">
              <a href="https://facultyprofiles.hkust.edu.hk/profiles.php?profile=yike-guo-yikeguo">Yike Guo</a><sup>2</sup><sup><sup>&#9993;</sup></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tencent RoboticsX,</span>
            <span class="author-block"><sup>2</sup>Hong Kong University of Science and Technology</span>
            <span class="author-block"><sup>3</sup>Peking University</span>
            <span class="author-block"><sup>*</sup>Equal contribution</span>
            <span class="author-block"><sup><sup>&#8224;</sup></sup>Project Lead</span>
            <span class="author-block"><sup><sup>&#9993;</sup></sup>Corresponding Author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.jpg" alt="Teaser Image" width="100%" />
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">MinD</span> is a hierarchical world model that unifies visual imagination and control
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<style>
  section {
    margin-bottom: -50px; /* Adjust this value to reduce vertical spacing */
  }
</style>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video generation models (VGMs) offer a promising pathway for unified world modeling in robotics by integrating simulation, prediction, and manipulation. However, their practical application remains limited due to (1) slowgeneration speed, which limits real-time interaction, and (2) poor consistency between imagined videos and executable actions.
          </p>
          <p>
            To address these challenges, we propose <strong>Manipulate in Dream (MinD)</strong>, a hierarchical diffusion-based world model framework that employs a dual-system design for vision-language manipulation. MinD executes VGM at low frequencies to extract video prediction features, while leveraging a high-frequency diffusion policy for real-time interaction. This architecture enables low-latency, closed-loop control in manipulation with coherent visual guidance.
          </p>
          <p>
            To better coordinate the two systems, we introduce a video-action diffusion matching module (DiffMatcher), with a novel co-training strategy that uses separate schedulers for each diffusion model. Specifically, we introduce a diffusion-forcing mechanism to DiffMatcher that aligns their intermediate representations during training, helping the fast action model better understand video-based predictions. Beyond manipulation, MinD also functions as a world simulator, reliably predicting task success or failure in latent space before execution. Trustworthy analysis further shows that VGMs can preemptively evaluate task feasibility and mitigate risks. Extensive experiments across multiple benchmarks demonstrate that MinD achieves state-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of unified world modeling in robotics.
            </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
      <div class="container is-max-desktop">
        <h2 class="title is-3">The MinD Model</h2>
        <div class="publication-image">
          <img src="./static/images/method_main.jpg" alt="Overview Image" style="width:100%;"/>
        </div>
      </div>
      <div class="content has-text-justified">
        <p>MinD is a general-purpose multimodal world model for robotic manipulation that integrates visual imagination and action planning. Its core consists of a hierarchical diffusion-based framework with three components:
        </p>
        <ol>
          <li><b>LoDiff-Visual (Slow System):</b> Generates a sequence of future visual observations at a low temporal frequency
            using a latent diffusion model, focusing on long-horizon imagination.</li>
          <li><b>HiDiff-Policy (Fast System):</b> Predicts high-frequency action sequences from the generated video rollout
            using a high-frequency diffusion transformer, ensuring real-time responsiveness.</li>
          <li><b>Video-Action DiffMatcher:</b> A temporal alignment module that bridges the asynchronous generation by
            converting latent video tensors into temporally-aware visual tokens, which then condition the HiDiff-Policy.</li>
        </ol>
        <p>During inference, LoDiff-Visual forward-simulates noisy visual latents, which DiffMatcher transforms into aligned
          features to condition HiDiff-Policy for action generation. The system is trained with a dual-scheduler co-training
          strategy, optimizing a total objective that includes a video loss, an action loss, and a regularization loss for
          DiffMatcher (to enforce consistency between noisy and clean visual features), ensuring robust performance across
          asynchronous temporal scales and imperfect inputs.
        </p>
        </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="column is-full-width">
      <h3 class="title is-3" style="margin-top: -30pt;">Experimental Results</h3>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
        <tr>
          <td width="9%"></td>
          <td width="82%">
            <img src="./static/images/exp_main.jpg" />
            <p class="has-text-centered" style="margin-top: -0.5em;"> Comparison of video generation result from LoDiff-Visual against real execution observation of HiDiff-Policy of RL-Bench and real-world Franka. </p>
          </td>
          <td width="9%"></td>
        </tr>
      </table>
      <h3 class="title is-4">Evaluation on RL-Bench</h3>
      <div class="content has-text-justified has-text-centered">
        <p>
          We first evaluate MinD in the <a href="https://github.com/stepjam/RLBench" target="_blank">RL-Bench</a> evaluation environment. This simulation platform is a comprehensive robot learning benchmark and environment with 7 tasks designed to advance research in vision-guided robot manipulation with a single-arm Franka Panda robot and a front-view camera. We compare MinD with existing VLA models, including <a href="https://arxiv.org/abs/2411.19650" target="_blank">CogACT</a>, <a href="https://arxiv.org/abs/2406.04339" target="_blank">RoboMamba</a>, <a href="https://arxiv.org/html/2404.12377v1" target="_blank">RoboDreamer</a>, and <a href="https://arxiv.org/abs/2406.09246" target="_blank">OpenVLA</a>. For the model using Mamba or LLM as the backbone, we colored it with a green background. We use a yellow background for the VLA models with a video generation backbone, and a red background for our method.
        </p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
          <tr>
            <td width="12%"></td>
            <td width="76%">
              <img src="static/images/exp_rlbench.png" />
              <p class="has-text-centered" style="margin-top: -0.5em;">
                Evaluation and comparison on RL-Bench tasks. All models are finetuned on the collected 1000 trajectories <span style="font-size: 80%;">(including 100 trajectories for each 7 tasks and 300 more randomly sampled tasks).</span>
              </p>
            </td>
            <td width="12%"></td>
          </tr>
        </table>
        <p>
            The results show that MinD outperforms all the existing VLA models, especially in tasks requiring complex temporal reasoning, such as "<b>Sweep to Dustpan</b>" (<span style="font-weight:bold;">96%</span>) and "<b>Close Laptop Lid</b>" (<span style="font-weight:bold;">68%</span>), highlighting the strong capability of video generation models as the foundational backbone for comprehensive visual-language manipulations. Besides, MinD also achieves the highest inference speed of <b>11.3</b> FPS, showcasing its superior efficiency.
        </p>
      </div>

      <h3 class="title is-4">Real-world Evaluation with Franka Research 3 Robot</h3>
      <div class="content has-text-justified has-text-centered">
        <p>
          We evaluate MinD with a <a href="https://franka.de/franka-research-3" target="_blank">Franka Research 3 Robot</a> to perform 4 real-world tasks: 1) pick and place, 2) unplug the charger, 3)
          pour water, 4) wipe the whiteboard. We collected a dataset with 100 human demonstration trajectories via teleoperation using a SpaceMouse. As shown in the table below, our model achieves competitive performance across all tasks, with notable strengths in tasks requiring precise manipulation, such as pick and place(60%) and wiping the whiteboard (65%).
        </p>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
          <tr>
            <td width="9%"></td>
            <td width="82%">
              <img src="./static/images/exp_franka.png" />
              <p class="has-text-centered" style="margin-top: -0.5em;"> Real-world evaluation with the Franka robot across four tasks, each with 20 trials of random configurations.</p>
            </td>
            <td width="9%"></td>
          </tr>
        </table>
      </div>

      <h3 class="title is-3" style="margin-top: 60px;">Video Result Samples</h3>
      <h4 class="title is-4">Franka Panda Robot <span style="font-size: 70%;">(in RL-Bench)</span></h4>
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve has-text-centered">
            <video poster="" id="steve video" autoplay controls muted loop playsinline>
              <source src="static/videos/realman/realman_u3.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-steve has-text-centered">
            <video poster="" id="steve video" autoplay controls muted loop playsinline>
              <source src="static/videos/realman/realman_u5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/realman/realman_u1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/realman/realman_u6.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody has-text-centered">
            <video poster="" id="fullbody video" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/realman/realman_u2.mp4" type="video/mp4">
              <track kind="subtitles" src="static/vtt/clamp.vtt" srclang="en" label="English" default>
            </video>
          </div>
          <div class="item item-shiba has-text-centered">
            <video poster="" id="shiba video" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/realman/realman_u7.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div style="width: 100%; text-align: center;">Examples of the Franka robot executing tasks with our model in RL-Bench.
        </div>
      </div>
      <br />

      <h4 class="title is-4">Franka Research 3</h4>
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve has-text-centered">
            <video poster="" id="steve video" autoplay controls muted loop playsinline>
              <source src="./static/videos/real_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-steve has-text-centered">
            <video poster="" id="steve video" autoplay controls muted loop playsinline>
              <source src="static/videos/realman/realman_u5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/realman/realman_u1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/realman/realman_u6.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody has-text-centered">
            <video poster="" id="fullbody video" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/realman/realman_u2.mp4" type="video/mp4">
              <track kind="subtitles" src="static/vtt/clamp.vtt" srclang="en" label="English" default>
            </video>
          </div>
          <div class="item item-shiba has-text-centered">
            <video poster="" id="shiba video" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/realman/realman_u7.mp4" type="video/mp4">
            </video>
          </div>
        </div>
            <div style="width: 100%; text-align: center;">Examples of the Franka robot executing tasks with our model in real world. <em><span style="font-size: 80%;">(more samples coming soon!)</span></em>
          </div>
          </div>
        </div>
        <br />


      <h3 class="title is-3" style="margin-top: 30pt;">Ablation Study</h3>
      <h4 class="title is-5">Modality Configurations & Trainable Modules</h4>
      <div class="content has-text-justified has-text-centered">
      <p>
        We evaluate each configuration based on video generation quality (FVD [30]) and success rate (SR)
        in task execution. The results highlight the impact of key components such as LDP, diffusion modules
        (LoDiff, DiffMatcher, HiDiff), and loss functions (Lvideo, Lsim, Laction) on performance. It also shows that large-scale video data pretraining and diffusion modules are vital for improving how well our MinD robot framework executes and generates videos. In short, using both video and action data, pretraining, and all the loss functions are key for best results in robot learning.
      </p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
        <tr>
          <td width="9%"></td>
          <td width="82%">
            <img src="./static/images/ablation.png" />
            <p class="has-text-centered" style="margin-top: -0.5em;"> Ablation study results. SE denotes the state encoder, LDP represents large-scale data pretraining, A denotes action and V is video.</p>
          </td>
          <td width="9%"></td>
          </tr>
      </table>
      </div>



      <h4 class="title is-5">Case Study: Can Video Generation Enable Trustworthy VLA?</h4>
      <div class="content has-text-justified has-text-centered">
      <p>
        We also conducted case study exploring how video generation models (VGMs) enhance the trustworthiness of world-model-based VLA by enabling risk assessment and outcome prediction for robotic tasks. We demonstrate that VGMs can predict both successful and failed executions, offering actionable insights for safer real-world deployment. While effective, future work should aim to improve motion prediction and incorporate richer multimodal inputs for more robust and reliable VLA.
      </p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
        <tr>
          <td width="9%"></td>
          <td width="82%">
            <img src="./static/images/case_study.png" />
            <p class="has-text-centered" style="margin-top: -0.5em;"> The left panel shows the confusion matrix, highlighting prediction accuracy for task outcomes. The right panel
            visualizes a failing case (top) with trajectory misalignment and a successful case (bottom) with accurate prediction.</p>
          </td>
          <td width="9%"></td>
        </tr>
      </table>
      <br />
      </div>


    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre>
      <code>
@article{li2024cogact,
  title={CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation},
  author={Li, Qixiu and Liang, Yaobo and Wang, Zeyu and Luo, Lin and Chen, Xi and Liao, Mozheng and Wei, Fangyun and Deng, Yu and Xu, Sicheng and Zhang, Yizhong and others},
  journal={arXiv preprint arXiv:2411.19650},
  year={2024}
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p> Website adapted from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
              href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
              International</a>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
